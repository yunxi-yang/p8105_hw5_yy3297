---
title: "p8105_hw5_yy3297"
author: "Yunxi Yang"
date: "2022-11-15"
output: github_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(dplyr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

#### Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`.
To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file.
As a next step, I `map` over paths and import data using the `read_csv` function.
Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others.
The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables.

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
tidy_df
```

Finally, the code chunk below creates a plot showing individual data, faceted by group.

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average.
Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way.

#### Problem 2

-   Import data set of homicide-data.csv; Convert the blanks and "unknown" to na for standardization purpose without modifying the raw data values of data set.

```{r}
homicide_df <- read.csv(file = "data/homicide-data.csv", na = c(" ", "Unknown"))
```

-   Description about the raw data:

This data set included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.
The 50 police departments were selected based on the size of the city and their violent crime reported to the FBI in 2012.

Specifically speaking, this data set contains `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns, with each row representing the information collected for each victim. There are 52179 observations and 14 variables.
Variables include `r colnames(homicide_df)`.
The 'uid' variable is the id number of each victim.
The 'reported_date' is the case reporting date as recorded.
The 'victim_last' variable suggests the last name of victim.
The 'victim_first' variable suggests the first name of victim.
The 'victim_race' is a categorical variable suggesting the race of each victim.
The 'victim_age' suggests each victim's age.
The 'city','state' suggests the city and state where the crime cases happened.
The 'lat' and 'lon' suggests the specific location with latitude and longitude where the crime cases happened.
The 'disposition' suggests that the disposition status for each victim's case.

-   Create a new city_state variable (e.g. "Baltimore, MD"); I have also noticed that the age variable category is not appropriate, so I adjust it into a numerical variable here; I have also noticed that there is a mismatching data row between the city and state, "Tulsa" and "AL". As I am not sure which should be the correct recording, I decided to delete this row to clear any confusion for further calculation; I categorize the cases of losed without arrest or Open/No arrest into unsolved cases, and categorize else into solved cases.

```{r}
homicide_df = 
  homicide_df %>% 
  janitor::clean_names() %>%
  mutate(
    city_state = str_c(city, state, sep = ", ", collapse = NULL),
    victim_age = as.numeric(victim_age),
    case_unsolved = ifelse(disposition == "Closed without arrest" | disposition == "Open/No arrest", 1, 0)
  ) %>%
  relocate(city_state) %>%
  filter(city_state != "Tulsa, AL")
homicide_df
```

-   Then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is "Closed without arrest" or "Open/No arrest").

```{r}
count_homicide_df =
  homicide_df %>%
  group_by(city_state) %>%
  summarize(
    n_total_homicides = n(),
    n_unsolved_homicides = sum(case_unsolved)
  ) 
count_homicide_df %>%
  knitr::kable()
```

-   For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy data frame.

```{r}
baltimore_homicide_df = 
  count_homicide_df %>%
  filter(city_state == "Baltimore, MD")
baltimore_homicide_df

baltimore_prop_test = 
  prop.test(
    x = baltimore_homicide_df %>% pull(n_unsolved_homicides),
    n = baltimore_homicide_df %>% pull(n_total_homicides)
  )

baltimore_prop_test %>% 
  broom::tidy() %>% 
  knitr::kable()
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each.

Do this within a "tidy" pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy data frame with estimated proportions and CIs for each city.

Create a prop test function for general application to all cities' calculations.

```{r}
prop_test_function = function(count_homicide_df){
  
  city_prop_test = 
    prop.test(
      x = count_homicide_df %>% pull(n_unsolved_homicides),
      n = count_homicide_df %>% pull(n_total_homicides)
    )
  
  return(city_prop_test)
}

# prop_test_function(baltimore_homicide_df)
# check passed, the value calculated by function is aligned with the direct calculation using prop.test
```

Iterate across all cities

```{r, warning=FALSE}
all_results_df = 
  count_homicide_df %>% 
    nest(data = 2:3) %>% 
    mutate(
      test_results = map(data, prop_test_function),
      tidy_results = map(test_results, broom::tidy)
    ) %>% 
    select(city_state, tidy_results) %>% 
    unnest(tidy_results) %>% 
    select(city_state, estimate, starts_with('conf'))
all_results_df %>% 
  knitr::kable()
```

Create a plot that shows the estimates and CIs for each city and apply geom_errorbar to add error bars based on the upper and lower limits; Also, organize cities according to the proportion of unsolved homicides in ascending orders.

```{r}
all_results_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  theme(legend.position = "bottom") +
  labs(
    x = "City, State",
    y = "Proportion of Unsolved Homicides",
    title = "Estimated Proportion of Unsolved Homicides for Each City",
    caption = "Data from Washington Post"
  )
```


#### Problem 3

* Question: When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

* First, set the following design elements: Fix n=30; Fix σ=5; Set μ=0; Generate 5000 data sets from the model; x∼Normal[μ,σ]; For each dataset, save μ̂  and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

```{r}
sim = function(n = 30, mu = 0, sigma = 5) {
  
     x = rnorm(n, mean = mu, sd = sigma)
     t_test = t.test(x, conf.int = 0.95)
  
     return(t_test)
}

output = vector("list", 5000)
for (i in 1:5000) {
  
  output[[i]] = sim() %>% 
    broom::tidy() %>%
    select(estimate, p.value)
     
}

output %>% bind_rows()
```

* Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

```{r}

```


Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
```{r}

```

Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. 
```{r}

```

Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 
```{r}

```

Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

```{r}

```
